These files were creating using a video lecture from @Andrei-Aksionov.

The files do no represent my creation of new work. Rather, these files represent a proof of concept 
that I could follow and understand the steps needed to create a LLM so that I may better understand
various alignment techniques.

LLM_that_works.py represents the final model, and in its current interation outputs very weak
Shakespearian texts, and uses the core functions of a transformer model while also showcasing
the importance of self-attention.

practice_for_LLM.py represents my practice in creating the final model, where I was able to
better understand the role each chuck of code plays in the final version. Importantly, the
later parts of the model showcase how self-attention can be obtained in increasingly more
efficent techniques.

